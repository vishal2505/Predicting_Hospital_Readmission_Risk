# Deployment Guide: Airflow on EC2 + ECS Fargate Pipeline

This guide covers end-to-end deployment of the Airflow orchestration layer on EC2 and the data processing pipeline on ECS Fargate.

## Architecture Overview

- **EC2 Instance**: Runs three Airflow containers (init, webserver, scheduler) using Docker Compose
  - Uses SequentialExecutor with SQLite (lightweight demo; can upgrade to Postgres+LocalExecutor later)
  - Mounts DAGs from the GitHub repo
  - Reads config from `/opt/airflow/airflow.env` (generated by Terraform user_data)
- **ECS Fargate Tasks**: Execute bronze/silver/gold data processing
  - Triggered by Airflow ECSOperator
  - Run the same containerized PySpark pipeline with env toggles (RUN_BRONZE/SILVER/GOLD)
  - Write outputs to S3
- **ECR**: Stores your pipeline container image
- **IAM**: EC2 role to call ECS RunTask; ECS task roles for S3/CloudWatch Logs
- **CloudWatch Logs**: 7-day retention for ECS task logs

## Prerequisites

1. **AWS CLI** configured with credentials
2. **Terraform** >= 1.5
3. **Docker** installed locally (to build and push the image)
4. **SSH Key Pair** in AWS (for EC2 access; or use Session Manager)
5. **S3 Buckets** created:
   - `diab-readmit-123456-datamart` (for bronze/silver/gold outputs)
   - `diab-readmit-123456-model-registry` (for future training artifacts)

## Step 1: Deploy Infrastructure (Automated - One Command!)

From the project root:

```bash
cd infra/terraform/aws-ec2-airflow-ecs
terraform init
terraform apply -auto-approve \
  -var "datamart_base_uri=s3a://diab-readmit-123456-datamart/"
```

**What Terraform Does Automatically:**
- ✅ Creates EC2 instance (t3.small) with public IP, SSM Agent
- ✅ Installs Docker and starts Airflow services (init, webserver, scheduler)
- ✅ Creates ECR repository
- ✅ **Builds Docker image locally and pushes to ECR** (automated!)
- ✅ Creates ECS cluster and Fargate task definition
- ✅ Sets up Security groups (ports 22 and 8080)
- ✅ Configures IAM roles and policies
- ✅ Creates CloudWatch Logs group

**Outputs:**
- `airflow_url` - Airflow UI endpoint
- `ecr_repository_url` - ECR repo URL (image already pushed!)
- `ecs_cluster_name`, `ecs_task_definition` - used by the DAG

**Note:** The Docker image build/push happens automatically during `terraform apply`. You don't need to build it manually!

## Step 2: Access Airflow UI

Open the `airflow_url` from Terraform outputs in your browser:

```bash
cd infra/terraform/aws-ec2-airflow-ecs
terraform output airflow_url
```

Example: `http://13.22.106.25:8080`

**Login**:
- Username: `airflow`
- Password: `airflow`

(Created by `airflow-init` service on first boot.)

## Step 3: Trigger the DAG

1. In the Airflow UI, find the DAG: `diab_medallion_ecs`
2. Enable it (toggle on)
3. Trigger manually (play button)
4. Monitor task progress:
   - Bronze → Silver → Gold tasks run sequentially
   - ECS tasks visible in AWS Console → ECS → Clusters → Tasks
   - Logs in CloudWatch Logs: `/ecs/diab-readmit-demo`

## Step 4: Verify Outputs in S3

After the DAG succeeds, check your S3 bucket:

```bash
aws s3 ls s3://diab-readmit-123456-datamart/ --recursive --human-readable
```

Expected structure:
```
bronze/diabetes/1999-01-01/
bronze/diabetes/1999-02-01/
...
silver/diabetes/1999-01-01/
...
gold/label_store/
gold/feature_store/
```

## Updating Your Pipeline Code

The Docker image build is automated, but you need to trigger a rebuild when you change code:

**When you update Python files (`main.py`, `requirements.txt`, `Dockerfile`):**

```bash
cd infra/terraform/aws-ec2-airflow-ecs

# Terraform detects changes via file hashes and rebuilds automatically
terraform apply -auto-approve -var "datamart_base_uri=s3a://diab-readmit-123456-datamart/"
```

Terraform tracks changes to:
- `Dockerfile` 
- `requirements.txt`
- `main.py`

If any of these change, the Docker image will be rebuilt and pushed to ECR automatically!

## Cost Management

- **EC2 t3.small**: ~$15-$25/month if running 24/7; stop instance overnight to save ~50%
  - Stop: `aws ec2 stop-instances --instance-ids <instance-id>`
  - Start: `aws ec2 start-instances --instance-ids <instance-id>`
- **ECS Fargate**: Pay only when tasks run (~$0.03-$0.10 per run for 1vCPU/2GB/40min)
- **S3/CloudWatch Logs**: Pennies for demo data volumes
- **No NAT Gateway**: ECS tasks use public IPs to avoid $0.045/hour NAT cost

## Cleanup

Destroy all resources:

```bash
cd infra/terraform/aws-ec2-airflow-ecs
terraform destroy -auto-approve
```

Empty and delete S3 buckets if needed:

```bash
aws s3 rm s3://diab-readmit-123456-datamart/ --recursive
aws s3 rb s3://diab-readmit-123456-datamart
```

## Troubleshooting

### Airflow UI not reachable
- SSH/SSM to the instance:
  ```bash
  # Via Session Manager (no key needed)
  # AWS Console → Systems Manager → Session Manager → select instance
  
  # Or via SSH (if key pair configured)
  ssh -i ~/.ssh/your-key.pem ec2-user@<public-ip>
  ```
- Check services:
  ```bash
  cd /opt/airflow/repo
  sudo docker compose -f airflow-docker-compose.yaml ps
  sudo docker compose -f airflow-docker-compose.yaml logs airflow-webserver --tail 100
  ```
- Restart if needed:
  ```bash
  sudo docker compose -f airflow-docker-compose.yaml restart
  ```

### ECS tasks failing
- Check CloudWatch Logs: `/ecs/diab-readmit-demo`
- Verify image was pushed to ECR:
  ```bash
  aws ecr describe-images --repository-name diab-readmit-pipeline --region ap-southeast-1
  ```
- Check IAM task role has S3 permissions
- Confirm DATAMART_BASE_URI env var in `/opt/airflow/airflow.env` matches your bucket

### DAG not appearing
- SSH to instance, verify repo cloned:
  ```bash
  ls -la /opt/airflow/repo/airflow/dags/
  ```
- Check for import errors in Airflow logs:
  ```bash
  sudo docker compose -f /opt/airflow/repo/airflow-docker-compose.yaml logs airflow-scheduler --tail 200 | grep -i error
  ```

## Next Steps

- Add training DAG task using ECS or SageMaker
- Implement inference pipeline (batch or real-time)
- Set up model registry in S3 with versioning
- Add monitoring/alerting for DAG failures
- Upgrade Airflow to Postgres+LocalExecutor for better concurrency
