# Deployment Guide: Airflow on EC2 + ECS Fargate Pipeline

This guide covers end-to-end deployment of the Airflow orchestration layer on EC2 and the data processing pipeline on ECS Fargate.

## Architecture Overview

- **EC2 Instance**: Runs three Airflow containers (init, webserver, scheduler) using Docker Compose
  - Uses SequentialExecutor with SQLite (lightweight demo; can upgrade to Postgres+LocalExecutor later)
  - Mounts DAGs from the GitHub repo
  - Reads config from `/opt/airflow/airflow.env` (generated by Terraform user_data)
- **ECS Fargate Tasks**: Execute bronze/silver/gold data processing
  - Triggered by Airflow ECSOperator
  - Run the same containerized PySpark pipeline with env toggles (RUN_BRONZE/SILVER/GOLD)
  - Write outputs to S3
- **ECR**: Stores your pipeline container image
- **IAM**: EC2 role to call ECS RunTask; ECS task roles for S3/CloudWatch Logs
- **CloudWatch Logs**: 7-day retention for ECS task logs

## Prerequisites

1. **AWS CLI** configured with credentials
2. **Terraform** >= 1.5
3. **Docker** installed locally (to build and push the image)
4. **SSH Key Pair** in AWS (for EC2 access; or use Session Manager)
5. **S3 Buckets** created:
   - `diab-readmit-123456-datamart` (for bronze/silver/gold outputs)
   - `diab-readmit-123456-model-registry` (for future training artifacts)

## Step 1: Deploy Infrastructure (Automated - One Command!)

From the project root:

```bash
cd infra/terraform/aws-ec2-airflow-ecs
terraform init
terraform apply -auto-approve \
  -var "datamart_base_uri=s3a://diab-readmit-123456-datamart/"
```

**What Terraform Does Automatically:**
- ✅ Creates EC2 instance (t3.small) with public IP, SSM Agent
- ✅ Installs Docker and starts Airflow services (init, webserver, scheduler)
- ✅ Creates ECR repository
- ✅ **Builds Docker image locally and pushes to ECR** (automated!)
- ✅ Creates ECS cluster and Fargate task definition
- ✅ Sets up Security groups (ports 22 and 8080)
- ✅ Configures IAM roles and policies
- ✅ Creates CloudWatch Logs group

**Outputs:**
- `airflow_url` - Airflow UI endpoint
- `ecr_repository_url` - ECR repo URL (image already pushed!)
- `ecs_cluster_name`, `ecs_task_definition` - used by the DAG

**Note:** The Docker image build/push happens automatically during `terraform apply`. You don't need to build it manually!

## Step 2: Access Airflow UI

Open the `airflow_url` from Terraform outputs in your browser:

```bash
cd infra/terraform/aws-ec2-airflow-ecs
terraform output airflow_url
```

Example: `http://13.22.106.25:8080`

**Login**:
- Username: `airflow`
- Password: `airflow`

(Created by `airflow-init` service on first boot.)

## Step 3: Trigger the DAG

1. In the Airflow UI, find the DAG: `diab_medallion_ecs`
2. Enable it (toggle on)
3. Trigger manually (play button)
4. Monitor task progress:
   - Bronze → Silver → Gold tasks run sequentially
   - ECS tasks visible in AWS Console → ECS → Clusters → Tasks
   - Logs in CloudWatch Logs: `/ecs/diab-readmit-demo`

## Step 4: Verify Outputs in S3

After the DAG succeeds, check your S3 bucket:

```bash
aws s3 ls s3://diab-readmit-123456-datamart/ --recursive --human-readable
```

Expected structure:
```
bronze/diabetes/1999-01-01/
bronze/diabetes/1999-02-01/
...
silver/diabetes/1999-01-01/
...
gold/label_store/
gold/feature_store/
```

## Updating Your Pipeline Code

### Option 1: Automated Build (Recommended)

The Docker image build is automated via Terraform. When you change code:

```bash
cd infra/terraform/aws-ec2-airflow-ecs

# Terraform detects changes via file hashes and rebuilds automatically
terraform apply -auto-approve -var "datamart_base_uri=s3a://diab-readmit-123456-datamart/"
```

**Terraform tracks changes to:**
- `Dockerfile` 
- `requirements.txt`
- `main.py`

If any of these files change, the Docker image will be rebuilt and pushed to ECR automatically!

### Option 2: Manual Build and Push

If you need to build and push the Docker image manually (for testing or quick iterations):

**Step 1: Get ECR repository URL**
```bash
# From project root
cd infra/terraform/aws-ec2-airflow-ecs
ECR_REPO=$(terraform output -json | jq -r '.ecr_repository_url.value')
echo "ECR Repository: $ECR_REPO"

# Or directly from AWS
ECR_REPO=$(aws ecr describe-repositories \
  --repository-names diab-readmit-pipeline \
  --region ap-southeast-1 \
  --query 'repositories[0].repositoryUri' \
  --output text)
```

**Step 2: Authenticate Docker to ECR**
```bash
aws ecr get-login-password --region ap-southeast-1 | \
  docker login --username AWS --password-stdin $ECR_REPO
```

**Step 3: Build the Docker image**
```bash
# Navigate to project root (where Dockerfile is)
cd ../../../

# Build the image
docker build -t diab-readmit-pipeline:latest .

# Tag for ECR
docker tag diab-readmit-pipeline:latest $ECR_REPO:latest

# Optional: Add timestamp tag for versioning
TIMESTAMP=$(date +%Y%m%d-%H%M%S)
docker tag diab-readmit-pipeline:latest $ECR_REPO:$TIMESTAMP
```

**Step 4: Push to ECR**
```bash
# Push latest tag
docker push $ECR_REPO:latest

# Push timestamp tag (optional)
docker push $ECR_REPO:$TIMESTAMP
```

**Step 5: Update task definition (if needed)**
```bash
# Terraform will use the :latest tag by default
# But you can force a new task definition revision:
cd infra/terraform/aws-ec2-airflow-ecs
terraform taint aws_ecs_task_definition.pipeline
terraform apply -auto-approve -var "datamart_base_uri=s3a://diab-readmit-123456-datamart/"
```

**All-in-One Script:**
```bash
#!/bin/bash
# Quick script to build and push Docker image manually

set -e

# Get ECR repo URL
ECR_REPO=$(aws ecr describe-repositories \
  --repository-names diab-readmit-pipeline \
  --region ap-southeast-1 \
  --query 'repositories[0].repositoryUri' \
  --output text)

echo "ECR Repository: $ECR_REPO"

# Login to ECR
echo "Authenticating to ECR..."
aws ecr get-login-password --region ap-southeast-1 | \
  docker login --username AWS --password-stdin $ECR_REPO

# Build image
echo "Building Docker image..."
docker build -t diab-readmit-pipeline:latest .

# Tag for ECR
echo "Tagging image..."
docker tag diab-readmit-pipeline:latest $ECR_REPO:latest

# Push to ECR
echo "Pushing to ECR..."
docker push $ECR_REPO:latest

echo "✅ Image pushed successfully: $ECR_REPO:latest"
```

Save this as `ops/build_push_docker.sh` and run:
```bash
chmod +x ops/build_push_docker.sh
./ops/build_push_docker.sh
```

### Updating Airflow DAGs

DAGs are pulled from Git on the EC2 instance. To update:

```bash
# SSH to EC2
ssh ec2-user@<PUBLIC_IP>

# Pull latest code
cd /opt/airflow/repo
git pull origin feature/airflow_aws_pipeline

# Restart Airflow to reload DAGs
docker compose -f airflow-docker-compose.yaml down
docker compose -f airflow-docker-compose.yaml up -d
```

**Important:** 
- **Pipeline code** (main.py, utils/) → Needs Docker rebuild and push to ECR
- **DAG code** (airflow/dags/) → Needs git pull on EC2 and Airflow restart
- **Both are separate!** DAG changes don't require Docker rebuild

## Cost Management

- **EC2 t3.small**: ~$15-$25/month if running 24/7; stop instance overnight to save ~50%
  - Stop: `aws ec2 stop-instances --instance-ids <instance-id>`
  - Start: `aws ec2 start-instances --instance-ids <instance-id>`
- **ECS Fargate**: Pay only when tasks run (~$0.03-$0.10 per run for 1vCPU/2GB/40min)
- **S3/CloudWatch Logs**: Pennies for demo data volumes
- **No NAT Gateway**: ECS tasks use public IPs to avoid $0.045/hour NAT cost

## Cleanup

Destroy all resources:

```bash
cd infra/terraform/aws-ec2-airflow-ecs
terraform destroy -auto-approve
```

Empty and delete S3 buckets if needed:

```bash
aws s3 rm s3://diab-readmit-123456-datamart/ --recursive
aws s3 rb s3://diab-readmit-123456-datamart
```

## Troubleshooting

### Airflow UI not reachable
- SSH/SSM to the instance:
  ```bash
  # Via Session Manager (no key needed)
  # AWS Console → Systems Manager → Session Manager → select instance
  
  # Or via SSH (if key pair configured)
  ssh -i ~/.ssh/your-key.pem ec2-user@<public-ip>
  ```
- Check services:
  ```bash
  cd /opt/airflow/repo
  sudo docker compose -f airflow-docker-compose.yaml ps
  sudo docker compose -f airflow-docker-compose.yaml logs airflow-webserver --tail 100
  ```
- Restart if needed:
  ```bash
  sudo docker compose -f airflow-docker-compose.yaml restart
  ```

### ECS tasks failing
- Check CloudWatch Logs: `/ecs/diab-readmit-demo`
- Verify image was pushed to ECR:
  ```bash
  aws ecr describe-images --repository-name diab-readmit-pipeline --region ap-southeast-1
  ```
- Check IAM task role has S3 permissions
- Confirm DATAMART_BASE_URI env var in `/opt/airflow/airflow.env` matches your bucket

### DAG not appearing
- SSH to instance, verify repo cloned:
  ```bash
  ls -la /opt/airflow/repo/airflow/dags/
  ```
- Check for import errors in Airflow logs:
  ```bash
  sudo docker compose -f /opt/airflow/repo/airflow-docker-compose.yaml logs airflow-scheduler --tail 200 | grep -i error
  ```

## Next Steps

- Add training DAG task using ECS or SageMaker
- Implement inference pipeline (batch or real-time)
- Set up model registry in S3 with versioning
- Add monitoring/alerting for DAG failures
- Upgrade Airflow to Postgres+LocalExecutor for better concurrency
