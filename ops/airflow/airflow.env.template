# Template for Airflow environment (all services)
# This file is populated by Terraform user_data with dynamic values
# Static Spark hints are pre-filled; ECS/DAG values are injected at boot

# Spark/Hadoop S3 classpath hints (for any local Spark usage from operators)
SPARK_CONF_DIR=/opt/airflow/repo/conf
SPARK_CLASSPATH=/opt/spark/jars-extra/hadoop-aws-3.3.4.jar:/opt/spark/jars-extra/aws-java-sdk-bundle-1.12.639.jar
SPARK_DRIVER_CLASS_PATH=/opt/spark/jars-extra/hadoop-aws-3.3.4.jar:/opt/spark/jars-extra/aws-java-sdk-bundle-1.12.639.jar

# Airflow logging
AIRFLOW__LOGGING__LOGGING_LEVEL=INFO

# Dynamic values for ECSOperator DAG (populated by Terraform)
AWS_REGION=__AWS_REGION__
ECS_CLUSTER=__ECS_CLUSTER__

# Task Definitions (separate for data processing vs model training)
ECS_TASK_DEF=__ECS_TASK_DEF__                           # Data processing (1vCPU/2GB)
ECS_MODEL_TRAINING_TASK_DEF=__ECS_MODEL_TRAINING_TASK_DEF__  # Model training (2vCPU/4GB)

ECS_SUBNETS=__ECS_SUBNETS__
ECS_SECURITY_GROUPS=__ECS_SECURITY_GROUPS__
DATAMART_BASE_URI=__DATAMART_BASE_URI__
MODEL_CONFIG_S3_PATH=s3://diab-readmit-123456-datamart/config/model_config.json
ECS_CONTAINER_NAME=__ECS_CONTAINER_NAME__
START_DATE=__START_DATE__
END_DATE=__END_DATE__
