# Spark defaults to avoid re-downloading S3 connector JARs and to set S3A defaults.
# These settings apply to all Spark sessions when SPARK_CONF_DIR points to this folder.

# Ship S3 connector and AWS SDK bundle baked into the image
spark.jars                       /opt/spark/jars-extra/hadoop-aws-3.3.4.jar,/opt/spark/jars-extra/aws-java-sdk-bundle-1.12.639.jar

# Use S3A filesystem
spark.hadoop.fs.s3a.impl         org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.path.style.access true
spark.hadoop.fs.s3a.fast.upload  true

# Credentials provider chain (env vars, profiles, role)
spark.hadoop.fs.s3a.aws.credentials.provider com.amazonaws.auth.EnvironmentVariableCredentialsProvider,com.amazonaws.auth.profile.ProfileCredentialsProvider,com.amazonaws.auth.InstanceProfileCredentialsProvider,com.amazonaws.auth.WebIdentityTokenCredentialsProvider

# Endpoint can be set dynamically in code based on region if needed; default global works for most cases.
# spark.hadoop.fs.s3a.endpoint    s3.amazonaws.com
