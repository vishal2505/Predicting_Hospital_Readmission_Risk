# Model Training Pipeline Setup

## Overview

The model training pipeline is now ready with temporal window validation and prerequisite checks. It follows MLOps best practices with proper data splitting to prevent leakage.

## Architecture

```
DAG: diab_model_training
‚îÇ
‚îú‚îÄ‚îÄ check_model_config (ShortCircuitOperator)
‚îÇ   ‚îî‚îÄ‚îÄ Validates model_config.json
‚îÇ
‚îú‚îÄ‚îÄ check_gold_data_exists (ShortCircuitOperator)
‚îÇ   ‚îî‚îÄ‚îÄ Ensures feature_store + label_store exist
‚îÇ
‚îú‚îÄ‚îÄ check_sufficient_training_data (ShortCircuitOperator)
‚îÇ   ‚îî‚îÄ‚îÄ Verifies >= 10 data partitions
‚îÇ
‚îî‚îÄ‚îÄ train_models (EcsRunTaskOperator)
    ‚îî‚îÄ‚îÄ Runs model_train.py on ECS Fargate
```

## Temporal Window Configuration

**File:** `conf/model_config.json`

The pipeline uses a 3-window temporal split:

| Window | Date Range | Purpose | Duration |
|--------|-----------|---------|----------|
| **Train** | 1999-01-01 to 2005-12-31 | Model fitting | 7 years |
| **Test** | 2006-01-01 to 2007-12-31 | Hyperparameter tuning & evaluation | 2 years |
| **OOT** | 2008-01-01 to 2008-12-31 | Final validation & inference | 1 year |

### Why This Split?

- **Prevents data leakage:** Strict temporal ordering
- **Realistic evaluation:** OOT simulates future unseen data
- **Sufficient training data:** 7 years provides robust signal
- **Proper validation:** 2-year test window for reliable performance estimation

## Training Configuration

### Algorithms Supported
1. **Logistic Regression** (baseline)
   - Fast, interpretable
   - Good for understanding feature importance
   
2. **Random Forest**
   - Handles non-linear relationships
   - Feature importance via tree splits
   
3. **XGBoost**
   - State-of-the-art gradient boosting
   - Best performance typically

### Hyperparameter Tuning
- Method: RandomizedSearchCV
- Cross-validation: 5-fold
- Scoring metric: AUC-ROC
- Iterations: 20 per algorithm

### Evaluation Metrics
- Accuracy
- Precision
- Recall
- F1-score
- AUC-ROC (primary metric)

## Files Created/Modified

### New Files
1. **`conf/model_config.json`**
   - Temporal window definitions
   - Model hyperparameters
   - Registry configuration

2. **`model_train.py`**
   - Main training script
   - Loads from gold layer
   - Trains multiple algorithms
   - Saves to S3 model registry

3. **`airflow/dags/diab_model_training.py`**
   - Training DAG with prerequisite checks
   - ShortCircuitOperators for validation
   - ECS task for training execution

### Modified Files
4. **`Dockerfile`**
   - Added model_train.py copy

## Model Registry Structure

Models are saved to S3 with the following structure:

```
s3://diab-readmit-123456-model-registry/
‚îî‚îÄ‚îÄ models/
    ‚îî‚îÄ‚îÄ readmission/
        ‚îú‚îÄ‚îÄ logistic_regression_v20251031_120000.pkl
        ‚îú‚îÄ‚îÄ logistic_regression_v20251031_120000_metadata.json
        ‚îú‚îÄ‚îÄ logistic_regression_latest.pkl  (symlink)
        ‚îú‚îÄ‚îÄ logistic_regression_latest_metadata.json
        ‚îú‚îÄ‚îÄ random_forest_v20251031_120500.pkl
        ‚îú‚îÄ‚îÄ random_forest_v20251031_120500_metadata.json
        ‚îú‚îÄ‚îÄ xgboost_v20251031_121000.pkl
        ‚îî‚îÄ‚îÄ xgboost_v20251031_121000_metadata.json
```

### Metadata Structure

Each model has a metadata JSON file containing:
```json
{
  "model_name": "xgboost",
  "training_date": "2025-10-31T12:10:00",
  "temporal_splits": {...},
  "feature_count": 45,
  "training_samples": 50000,
  "test_samples": 15000,
  "oot_samples": 8000,
  "performance": {
    "test": {
      "accuracy": 0.72,
      "precision": 0.68,
      "recall": 0.75,
      "f1": 0.71,
      "auc_roc": 0.78
    },
    "oot": {
      "accuracy": 0.70,
      "precision": 0.66,
      "recall": 0.73,
      "f1": 0.69,
      "auc_roc": 0.76
    }
  },
  "config": {...}
}
```

## How to Run

### Prerequisites
1. ‚úÖ Data processing DAG completed (diab_medallion_ecs)
2. ‚úÖ Gold layer data in S3 (feature_store + label_store)
3. ‚úÖ S3 bucket for model registry created

### Step 1: Create Model Registry Bucket (if not exists)

```bash
aws s3 mb s3://diab-readmit-123456-model-registry --region ap-southeast-1
```

### Step 2: Update Airflow DAG on EC2

```bash
# SSH to EC2
ssh ec2-user@<EC2_PUBLIC_IP>

# Pull latest code
cd /opt/airflow/repo
git pull origin feature/airflow_aws_pipeline

# Restart Airflow
docker compose -f airflow-docker-compose.yaml down
docker compose -f airflow-docker-compose.yaml up -d
```

### Step 3: Build and Push Docker Image

```bash
# From local machine
ECR_REPO=$(aws ecr describe-repositories \
  --repository-names diab-readmit-pipeline \
  --region ap-southeast-1 \
  --query 'repositories[0].repositoryUri' \
  --output text)

aws ecr get-login-password --region ap-southeast-1 | \
  docker login --username AWS --password-stdin ${ECR_REPO}

docker build -t diab-readmit-pipeline:latest .
docker tag diab-readmit-pipeline:latest ${ECR_REPO}:latest
docker push ${ECR_REPO}:latest
```

### Step 4: Trigger Training DAG

1. Open Airflow UI: `http://<EC2_PUBLIC_IP>:8080`
2. Find DAG: `diab_model_training`
3. Enable the DAG (toggle on)
4. Trigger manually (play button)

### Step 5: Monitor Training

**In Airflow UI:**
- Watch task progress: check_model_config ‚Üí check_gold_data_exists ‚Üí check_sufficient_training_data ‚Üí train_models

**In AWS Console:**
- ECS ‚Üí Clusters ‚Üí diab-readmit-demo-cluster ‚Üí Tasks
- CloudWatch Logs ‚Üí /ecs/diab-readmit-demo

**Expected Runtime:** 1-2 hours (depends on data volume and algorithms)

## Validation Checks

### 1. Model Config Check
```python
# Validates:
- temporal_splits exist
- All required fields present
- start_date/end_date for train/test/oot
```

**Skip if:** Config is invalid or missing

### 2. Gold Data Exists Check
```python
# Validates:
- s3://.../gold/feature_store/ has files
- s3://.../gold/label_store/ has files
```

**Skip if:** Data doesn't exist (run diab_medallion_ecs first)

### 3. Sufficient Training Data Check
```python
# Validates:
- At least 10 parquet partitions in feature_store
```

**Skip if:** Insufficient data partitions

## Troubleshooting

### Issue: DAG skipped all tasks

**Cause:** One of the ShortCircuitOperator checks failed

**Solution:**
1. Check Airflow logs for which check failed
2. Fix the underlying issue:
   - Config invalid ‚Üí Fix conf/model_config.json
   - Data missing ‚Üí Run diab_medallion_ecs DAG
   - Insufficient data ‚Üí Extend data processing date range

### Issue: Training task fails with "No module named 'sklearn'"

**Cause:** Docker image doesn't have scikit-learn

**Solution:**
Add to requirements.txt:
```
scikit-learn==1.5.1
xgboost==2.1.0
```

Then rebuild and push image.

### Issue: "Access Denied" when saving to S3

**Cause:** ECS task role doesn't have S3 write permissions

**Solution:**
Update task role policy in Terraform to include:
```json
{
  "Effect": "Allow",
  "Action": [
    "s3:PutObject",
    "s3:GetObject"
  ],
  "Resource": "arn:aws:s3:::diab-readmit-123456-model-registry/*"
}
```

## Next Steps

After training completes:

1. ‚úÖ **Validate Models** - Check model performance in metadata
2. ‚è≠Ô∏è **Inference DAG** - Use trained models for batch predictions
3. ‚è≠Ô∏è **Monitoring DAG** - Track model drift and performance
4. ‚è≠Ô∏è **Model Comparison** - Compare multiple model versions
5. ‚è≠Ô∏è **A/B Testing** - Test models in production

## Configuration Customization

### Modify Temporal Windows

Edit `conf/model_config.json`:
```json
{
  "temporal_splits": {
    "train": {
      "start_date": "2000-01-01",  // Adjust as needed
      "end_date": "2006-12-31"
    },
    ...
  }
}
```

### Change Algorithms

Edit `conf/model_config.json`:
```json
{
  "training_config": {
    "algorithms": ["xgboost"],  // Train only XGBoost
    "hyperparameter_tuning": true
  }
}
```

### Adjust Hyperparameter Search

Modify `model_train.py` - Update param_dist dictionaries in each training function.

## Summary

‚úÖ **Temporal window configuration**: Prevents data leakage  
‚úÖ **Prerequisite checks**: Ensures safe execution  
‚úÖ **Multiple algorithms**: LogReg, RF, XGBoost  
‚úÖ **Hyperparameter tuning**: Automated with RandomizedSearchCV  
‚úÖ **Model registry**: Versioned storage in S3  
‚úÖ **Metadata tracking**: Full training lineage  

**Your training pipeline is production-ready!** üöÄ
