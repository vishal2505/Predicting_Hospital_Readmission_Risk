# Model Training Pipeline Setup

## Overview

The model training pipeline supports **parallel training of multiple algorithms** with temporal window validation and prerequisite checks. Each algorithm runs as a separate ECS task for independent execution, failure handling, and resource optimization.

**Key Infrastructure:** Uses a **dedicated ECS task definition** with **2 vCPU / 4 GB** (double the resources of data processing) for faster model training.

## Architecture

```
DAG: diab_model_training
â”‚
â”œâ”€â”€ Prerequisite Checks (Sequential)
â”‚   â”œâ”€â”€ check_model_config (ShortCircuitOperator)
â”‚   â”‚   â””â”€â”€ Validates model_config.json
â”‚   â”œâ”€â”€ check_gold_data_exists (ShortCircuitOperator)
â”‚   â”‚   â””â”€â”€ Ensures feature_store + label_store exist
â”‚   â””â”€â”€ check_sufficient_training_data (ShortCircuitOperator)
â”‚       â””â”€â”€ Verifies >= 10 data partitions
â”‚
â”œâ”€â”€ Data Preprocessing (Runs ONCE, 30min timeout)
â”‚   â””â”€â”€ preprocess_training_data (ECS Task - 2vCPU/4GB)
â”‚       â”œâ”€â”€ Load gold layer (feature_store + label_store)
â”‚       â”œâ”€â”€ Apply temporal splits (train/test/oot)
â”‚       â”œâ”€â”€ Apply StandardScaler preprocessing
â”‚       â””â”€â”€ Save to s3://bucket/gold/preprocessed/
â”‚
â””â”€â”€ Model Training (Parallel, loads preprocessed data)
    â”œâ”€â”€ train_logistic_regression (2vCPU/4GB, 2hr timeout)
    â”œâ”€â”€ train_random_forest (2vCPU/4GB, 3hr timeout)
    â””â”€â”€ train_xgboost (2vCPU/4GB, 3hr timeout)
```

**Key Optimization:** Preprocessing runs ONCE and saves to S3. All 3 training tasks load the same preprocessed data, eliminating redundant preprocessing (3x efficiency improvement).


### ECS Task Definitions

The pipeline uses **two separate task definitions** for optimal resource allocation:

| Task Definition | CPU | Memory | Use Case | Tasks |
|----------------|-----|--------|----------|-------|
| **diab-readmit-demo-pipeline** | 1 vCPU | 2 GB | Data processing (Bronzeâ†’Silverâ†’Gold) | ETL workloads |
| **diab-readmit-demo-model-training** | 2 vCPU | 4 GB | Model training with hyperparameter tuning | ML workloads |

**Benefits:**
- ğŸš€ **2x faster training** with increased CPU
- ğŸ’¾ **Prevents OOM errors** with 4GB memory for large datasets
- ğŸ’° **Cost optimized** - data processing still uses cheaper 1vCPU tasks
- ğŸ”§ **Independent scaling** - adjust resources per workload type

### Key Design Decisions

**Parallel vs Sequential Training:**
- âœ… **Parallel:** Faster pipeline execution, independent failures, better resource utilization
- âŒ Sequential: Slower, one failure blocks all subsequent models

**Single vs Multi-Algorithm Mode:**
- **Single-Algorithm Mode:** Triggered by DAG tasks with `ALGORITHM` environment variable
- **Multi-Algorithm Mode:** Local/manual execution trains all enabled algorithms

**Separate Task Definitions:**
- âœ… **Model Training (2vCPU/4GB):** Optimized for CPU-intensive ML workloads
- âœ… **Data Processing (1vCPU/2GB):** Cost-effective for I/O-bound ETL tasks

## Temporal Window Configuration

**File:** `conf/model_config.json`

The pipeline uses a 3-window temporal split:

| Window | Date Range | Purpose | Duration |
|--------|-----------|---------|----------|
| **Train** | 1999-01-01 to 2005-12-31 | Model fitting | 7 years |
| **Test** | 2006-01-01 to 2007-12-31 | Hyperparameter tuning & evaluation | 2 years |
| **OOT** | 2008-01-01 to 2008-12-31 | Final validation & inference | 1 year |

### Why This Split?

- **Prevents data leakage:** Strict temporal ordering
- **Realistic evaluation:** OOT simulates future unseen data
- **Sufficient training data:** 7 years provides robust signal
- **Proper validation:** 2-year test window for reliable performance estimation

## Training Configuration

### Algorithms Supported

Each algorithm runs as an **independent ECS task** in the DAG:

1. **Logistic Regression** (2 hour timeout)
   - Fast, interpretable baseline
   - L1/L2 regularization
   - Class weight balancing
   - Good for understanding feature importance
   
2. **Random Forest** (3 hour timeout)
   - Handles non-linear relationships
   - Feature importance via tree splits
   - Tree depth and count optimization
   
3. **XGBoost** (3 hour timeout)
   - State-of-the-art gradient boosting
   - Learning rate optimization
   - Scale_pos_weight for class imbalance
   - Best performance typically

### Algorithm Toggle Configuration

**Enable/Disable algorithms via `conf/model_config.json`:**

```json
{
  "training_config": {
    "algorithms": ["logistic_regression", "random_forest", "xgboost"],
    "enabled_algorithms": {
      "logistic_regression": true,
      "random_forest": true,
      "xgboost": false  // Disable XGBoost by setting to false
    },
    "hyperparameter_tuning": true,
    "feature_selection": true,
    "class_weight": "balanced"
  }
}
```

**How it works:**
- **In DAG (ECS):** Each algorithm has a dedicated task that runs regardless of config (passes `ALGORITHM` env var)
- **In model_train.py:** Checks `ALGORITHM` env var; if set, trains only that algorithm
- **Local execution:** Reads `enabled_algorithms` from config and trains all enabled ones

### Execution Modes

1. **Single-Algorithm Mode (DAG)**
   ```python
   # Triggered by ECS task with environment variable
   ALGORITHM=logistic_regression python model_train.py
   ```
   - Used by Airflow DAG tasks
   - Each algorithm runs in parallel
   - Independent failures and resource allocation

2. **Multi-Algorithm Mode (Local)**
   ```bash
   # No ALGORITHM env var - trains all enabled algorithms
   python model_train.py
   ```
   - Used for local testing
   - Trains all algorithms with `enabled_algorithms: true`
   - Sequential execution

### Hyperparameter Tuning
- Method: RandomizedSearchCV
- Cross-validation: 5-fold
- Scoring metric: AUC-ROC
- Iterations: 20 per algorithm

### Evaluation Metrics
- Accuracy
- Precision
- Recall
- F1-score
- AUC-ROC (primary metric)

## S3 Data Structure

### Gold Layer (Input)

```
s3://bucket/gold/
â”œâ”€â”€ feature_store/                    # Created by data processing DAG
â”‚   â””â”€â”€ partition_date=YYYY-MM-DD/
â”‚       â””â”€â”€ *.parquet
â”œâ”€â”€ label_store/                      # Created by data processing DAG
â”‚   â””â”€â”€ partition_date=YYYY-MM-DD/
â”‚       â””â”€â”€ *.parquet
â””â”€â”€ preprocessed/                     # Created by preprocessing task
    â”œâ”€â”€ latest.txt                    # Points to latest preprocessing run
    â””â”€â”€ train_data_YYYYMMDD_HHMMSS/
        â”œâ”€â”€ train_processed.parquet   # Scaled training features + labels
        â”œâ”€â”€ test_processed.parquet    # Scaled test features + labels
        â”œâ”€â”€ oot_processed.parquet     # Scaled OOT features + labels
        â”œâ”€â”€ scaler.pkl                # Fitted StandardScaler object
        â””â”€â”€ metadata.json             # Data shapes, readmission rates, etc.
```

### Model Registry (Output)

Models are organized by algorithm with version tracking:

```
s3://bucket/model_registry/
â”œâ”€â”€ logistic_regression/
â”‚   â”œâ”€â”€ latest/                       # Always points to latest model
â”‚   â”‚   â”œâ”€â”€ model.pkl
â”‚   â”‚   â””â”€â”€ metadata.json
â”‚   â”œâ”€â”€ v20251101_155508/             # Versioned model
â”‚   â”‚   â”œâ”€â”€ model.pkl
â”‚   â”‚   â””â”€â”€ metadata.json
â”‚   â”œâ”€â”€ v20251101_140000/             # Previous version
â”‚   â”‚   â”œâ”€â”€ model.pkl
â”‚   â”‚   â””â”€â”€ metadata.json
â”‚   â””â”€â”€ versions.json                 # Index of all versions
â”œâ”€â”€ random_forest/
â”‚   â”œâ”€â”€ latest/
â”‚   â”œâ”€â”€ v20251101_155509/
â”‚   â””â”€â”€ versions.json
â””â”€â”€ xgboost/
    â”œâ”€â”€ latest/
    â”œâ”€â”€ v20251101_155510/
    â””â”€â”€ versions.json
```

**Benefits:**
- âœ… **Organized:** Clear folder per algorithm
- âœ… **Versioned:** Keep all historical models
- âœ… **Discoverable:** `latest/` always has current production model
- âœ… **Traceable:** `versions.json` has full version history with OOT AUC
- âœ… **Rollback-friendly:** Easy to revert to previous version

## Files Created/Modified

### New Files
1. **`conf/model_config.json`**
   - Temporal window definitions
   - Model hyperparameters
   - Registry configuration
   - Algorithm toggle (`enabled_algorithms`)

2. **`preprocess_train_data.py`** (NEW)
   - Loads gold layer (feature_store + label_store)
   - Applies temporal window splits
   - Applies StandardScaler preprocessing
   - Saves to `gold/preprocessed/` in S3

3. **`model_train.py`**
   - Loads preprocessed data from S3 (no redundant preprocessing)
   - Trains single algorithm (from ALGORITHM env var)
   - Saves to organized model registry structure

4. **`airflow/dags/diab_model_training.py`**
   - Training DAG with prerequisite checks
   - Preprocessing task (runs once)
   - 3 parallel training tasks
   - ShortCircuitOperators for validation

### Modified Files
5. **`Dockerfile`**
   - Added preprocess_train_data.py copy
   - Added model_train.py copy

6. **`requirements.txt`**
   - Added pyarrow>=18.0.0 for Parquet I/O

## Model Registry Structure (Legacy Reference)

Old flat structure (no longer used):

```
s3://diab-readmit-123456-model-registry/
â””â”€â”€ models/
    â””â”€â”€ readmission/
        â”œâ”€â”€ logistic_regression_v20251031_120000.pkl
        â”œâ”€â”€ logistic_regression_v20251031_120000_metadata.json
        â”œâ”€â”€ logistic_regression_latest.pkl  (symlink)
        â”œâ”€â”€ logistic_regression_latest_metadata.json
        â”œâ”€â”€ random_forest_v20251031_120500.pkl
        â”œâ”€â”€ random_forest_v20251031_120500_metadata.json
        â”œâ”€â”€ xgboost_v20251031_121000.pkl
        â””â”€â”€ xgboost_v20251031_121000_metadata.json
```

### Metadata Structure

Each model has a metadata JSON file containing:
```json
{
  "model_name": "xgboost",
  "training_date": "2025-10-31T12:10:00",
  "temporal_splits": {...},
  "feature_count": 45,
  "training_samples": 50000,
  "test_samples": 15000,
  "oot_samples": 8000,
  "performance": {
    "test": {
      "accuracy": 0.72,
      "precision": 0.68,
      "recall": 0.75,
      "f1": 0.71,
      "auc_roc": 0.78
    },
    "oot": {
      "accuracy": 0.70,
      "precision": 0.66,
      "recall": 0.73,
      "f1": 0.69,
      "auc_roc": 0.76
    }
  },
  "config": {...}
}
```

## How to Run

### Prerequisites
1. âœ… Data processing DAG completed (diab_medallion_ecs)
2. âœ… Gold layer data in S3 (feature_store + label_store)
3. âœ… S3 bucket for model registry created

### Step 1: Create Model Registry Bucket (if not exists)

```bash
aws s3 mb s3://diab-readmit-123456-model-registry --region ap-southeast-1
```

### Step 2: Update Airflow DAG on EC2

```bash
# SSH to EC2
ssh ec2-user@<EC2_PUBLIC_IP>

# Pull latest code
cd /opt/airflow/repo
git pull origin feature/airflow_aws_pipeline

# Restart Airflow
docker compose -f airflow-docker-compose.yaml down
docker compose -f airflow-docker-compose.yaml up -d
```

### Step 3: Build and Push Docker Image

```bash
# From local machine
ECR_REPO=$(aws ecr describe-repositories \
  --repository-names diab-readmit-pipeline \
  --region ap-southeast-1 \
  --query 'repositories[0].repositoryUri' \
  --output text)

aws ecr get-login-password --region ap-southeast-1 | \
  docker login --username AWS --password-stdin ${ECR_REPO}

docker build -t diab-readmit-pipeline:latest .
docker tag diab-readmit-pipeline:latest ${ECR_REPO}:latest
docker push ${ECR_REPO}:latest
```

### Step 4: Trigger Training DAG

1. Open Airflow UI: `http://<EC2_PUBLIC_IP>:8080`
2. Find DAG: `diab_model_training`
3. Enable the DAG (toggle on)
4. Trigger manually (play button)

### Step 5: Monitor Training

**In Airflow UI:**
- Watch task progress: 
  - `check_model_config` â†’ `check_gold_data_exists` â†’ `check_sufficient_training_data`
  - Then: `preprocess_training_data` (runs once)
  - Finally 3 parallel tasks: `train_logistic_regression`, `train_random_forest`, `train_xgboost`
- Graph View shows preprocessing followed by parallel execution of training tasks
- Each task can succeed/fail independently

**In AWS Console:**
- ECS â†’ Clusters â†’ diab-readmit-demo-cluster â†’ Tasks
- CloudWatch Logs â†’ /ecs/diab-readmit-demo-model-training
- Each algorithm will have separate task logs

**Expected Runtime:** 
- **Preprocessing:** ~5 minutes (runs once)
- **Parallel training:** ~2-2.5 hours (all models train simultaneously)
- **Total:** ~2.5-3 hours
- **vs Sequential:** Would take 6-8 hours (if tasks were chained)

### Step 6: Verify Preprocessed Data and Models

**Check Preprocessed Data:**
```bash
# Check latest preprocessing
aws s3 cat s3://diab-readmit-123456-datamart/gold/preprocessed/latest.txt

# List preprocessing artifacts
aws s3 ls s3://diab-readmit-123456-datamart/gold/preprocessed/train_data_20251101_120000/
# Expected:
# train_processed.parquet
# test_processed.parquet
# oot_processed.parquet
# scaler.pkl
# metadata.json
```

**Check Model Registry (Organized by Algorithm):**
```bash
# List all algorithms
aws s3 ls s3://diab-readmit-123456-model-registry/

# Check specific algorithm versions
aws s3 ls s3://diab-readmit-123456-model-registry/logistic_regression/
# Expected structure:
# latest/model.pkl
# latest/metadata.json
# v20251101_155508/model.pkl
# v20251101_155508/metadata.json
# versions.json

# View version history
aws s3 cp s3://diab-readmit-123456-model-registry/xgboost/versions.json -
```

## Task Dependencies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ check_model_config  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ check_gold_data_exists   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ check_sufficient_training_data  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ preprocess_training_data     â”‚
â”‚ (Loads, scales, saves to S3) â”‚
â”‚ 30min timeout                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚          â”‚
    â–¼             â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LogReg â”‚  â”‚ RandFor  â”‚  â”‚ XGBoost  â”‚
â”‚ 2hr    â”‚  â”‚ 3hr      â”‚  â”‚ 3hr      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits of This Architecture:**
1. â±ï¸ **Faster:** ~2.5 hours total (preprocessing once + parallel training)
2. ğŸ”„ **No redundant work:** Preprocessing runs once, shared by all models
3. ğŸ“Š **Consistent:** All models train on identical preprocessed data
4. ğŸ›ï¸ **Control:** Disable specific algorithms via config toggle
5. ğŸ› **Debuggable:** Can inspect preprocessed data before training

## Validation Checks

### 1. Model Config Check
```python
# Validates:
- temporal_splits exist
- All required fields present
- start_date/end_date for train/test/oot
```

**Skip if:** Config is invalid or missing

### 2. Gold Data Exists Check
```python
# Validates:
- s3://.../gold/feature_store/ has files
- s3://.../gold/label_store/ has files
```

**Skip if:** Data doesn't exist (run diab_medallion_ecs first)

### 3. Sufficient Training Data Check
```python
# Validates:
- At least 10 parquet partitions in feature_store
```

**Skip if:** Insufficient data partitions

## Troubleshooting

### Issue: DAG skipped all tasks

**Cause:** One of the ShortCircuitOperator checks failed

**Solution:**
1. Check Airflow logs for which check failed
2. Fix the underlying issue:
   - Config invalid â†’ Fix conf/model_config.json and re-upload to S3
   - Data missing â†’ Run diab_medallion_ecs DAG
   - Insufficient data â†’ Extend data processing date range

### Issue: Specific algorithm task fails

**Cause:** Algorithm-specific error (e.g., XGBoost memory issues)

**Solution:**
1. Check CloudWatch logs for the specific algorithm task
2. Options:
   - **Disable the failing algorithm:** Set `enabled_algorithms.xgboost: false` in config
   - **Increase resources:** Update ECS task definition with more CPU/memory
   - **Adjust hyperparameters:** Reduce complexity in model_train.py

**Benefits of separate tasks:** Other algorithms continue training successfully!

### Issue: Training task fails with "No module named 'sklearn'"

**Cause:** Docker image doesn't have scikit-learn

**Solution:**
Add to requirements.txt:
```
scikit-learn==1.5.1
xgboost==2.1.0
```

Then rebuild and push image.

### Issue: "Access Denied" when saving to S3

**Cause:** ECS task role doesn't have S3 write permissions

**Solution:**
Update task role policy in Terraform to include:
```json
{
  "Effect": "Allow",
  "Action": [
    "s3:PutObject",
    "s3:GetObject",
    "s3:ListBucket"
  ],
  "Resource": [
    "arn:aws:s3:::diab-readmit-123456-model-registry",
    "arn:aws:s3:::diab-readmit-123456-model-registry/*"
  ]
}
```

### Issue: "ValueError: could not convert string to float"

**Cause:** Categorical features (diagnosis codes) not excluded from training

**Solution:**
This is now fixed in the latest model_train.py:
- Excludes `diag_1`, `diag_2`, `diag_3` columns
- Applies StandardScaler to 7 numeric features only
- Uses ColumnTransformer with `remainder='passthrough'` for one-hot encoded features

### Issue: Want to train only one algorithm for testing

**Solution:**
**Option 1 - Disable in config (affects local execution only):**
```json
{
  "training_config": {
    "enabled_algorithms": {
      "logistic_regression": true,
      "random_forest": false,  // Skip this
      "xgboost": false          // Skip this
    }
  }
}
```

**Option 2 - Manual DAG task trigger (affects Airflow):**
In Airflow UI, manually trigger only specific tasks:
1. Run prerequisite checks
2. Trigger only `train_logistic_regression` task
3. Skip `train_random_forest` and `train_xgboost`

**Option 3 - Local testing:**
```bash
export ALGORITHM=logistic_regression
python model_train.py
```

## Local Test Results

The training pipeline was successfully tested locally in Docker with the following results:

### Dataset Statistics
```
âœ“ X_train: (71,596, 14), Readmission rate: 11.1%
âœ“ X_test:  (20,288, 14), Readmission rate: 11.2%
âœ“ X_oot:   (9,882, 14),  Readmission rate: 11.4%
```

### Preprocessing Applied
```
StandardScaler applied to 7 numeric features:
- age_midpoint
- admission_severity_score
- admission_source_risk_score
- metformin_ord
- insulin_ord
- severity_x_visits
- medication_density
```

### Model Performance

| Model | Test AUC | Test Recall | OOT AUC | OOT Recall | Training Time |
|-------|----------|-------------|---------|------------|---------------|
| **Logistic Regression** | 0.6120 | 0.4481 | 0.6161 | 0.4495 | ~2 min |
| **Random Forest** | 0.6202 | 0.4662 | **0.6308** | 0.4663 | ~3 min |
| **XGBoost** | 0.6198 | 0.4865 | **0.6321** | **0.4965** | ~3 min |

**Key Observations:**
- âœ… **No data leakage:** OOT performance consistent with Test set
- âœ… **XGBoost best:** Highest OOT AUC (0.6321) and recall (0.4965)
- âœ… **Class imbalance handled:** All models use balanced class weights
- âœ… **All models saved:** Successfully uploaded to S3 with metadata

**Best Hyperparameters:**
```python
# Logistic Regression
{'penalty': 'l1', 'class_weight': 'balanced', 'C': 0.001}

# Random Forest
{'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 5, 
 'min_samples_leaf': 1, 'class_weight': 'balanced'}

# XGBoost
{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50,
 'subsample': 1.0, 'colsample_bytree': 0.6, 'scale_pos_weight': 7.99}
```

## Next Steps

After training completes:

1. âœ… **Validate Models** - Check model performance in metadata
2. â­ï¸ **Inference DAG** - Use trained models for batch predictions
3. â­ï¸ **Monitoring DAG** - Track model drift and performance
4. â­ï¸ **Model Comparison** - Compare multiple model versions
5. â­ï¸ **A/B Testing** - Test models in production

## Configuration Customization

### Modify Temporal Windows

Edit `conf/model_config.json`:
```json
{
  "temporal_splits": {
    "train": {
      "start_date": "2000-01-01",  // Adjust as needed
      "end_date": "2006-12-31"
    },
    ...
  }
}
```

### Change Algorithms

Edit `conf/model_config.json`:
```json
{
  "training_config": {
    "algorithms": ["xgboost"],  // Train only XGBoost
    "hyperparameter_tuning": true
  }
}
```

### Adjust Hyperparameter Search

Modify `model_train.py` - Update param_dist dictionaries in each training function.

## Summary

âœ… **Temporal window configuration**: Prevents data leakage  
âœ… **Prerequisite checks**: Ensures safe execution  
âœ… **Multiple algorithms**: LogReg, RF, XGBoost  
âœ… **Hyperparameter tuning**: Automated with RandomizedSearchCV  
âœ… **Model registry**: Versioned storage in S3  
âœ… **Metadata tracking**: Full training lineage  

**Your training pipeline is production-ready!** ğŸš€
